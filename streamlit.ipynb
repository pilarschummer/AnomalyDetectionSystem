{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMEbPizG1FFtc5Ikl0mUKfh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install streamlit -q\n","!pip install streamlit-lottie\n","!pip install Pillow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__QVEnT84PvK","executionInfo":{"status":"ok","timestamp":1716817061632,"user_tz":-120,"elapsed":57004,"user":{"displayName":"Pilar Schümmer","userId":"07078196375778711023"}},"outputId":"8d91deed-42d5-4372-acd6-ba3d475641d1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting streamlit-lottie\n","  Downloading streamlit_lottie-0.0.5-py3-none-any.whl (802 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.4/802.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: streamlit>=0.63 in /usr/local/lib/python3.10/dist-packages (from streamlit-lottie) (1.35.0)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (4.2.2)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit>=0.63->streamlit-lottie) (1.4)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (5.3.3)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (8.1.7)\n","Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (1.25.2)\n","Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (24.0)\n","Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (2.0.3)\n","Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (9.4.0)\n","Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (3.20.3)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (14.0.2)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (2.31.0)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (13.7.1)\n","Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (8.3.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (4.11.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (3.1.43)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (0.9.1)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (6.3.3)\n","Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.63->streamlit-lottie) (4.0.1)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-lottie) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-lottie) (3.1.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-lottie) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=0.63->streamlit-lottie) (0.12.1)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-lottie) (4.0.11)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-lottie) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-lottie) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit>=0.63->streamlit-lottie) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-lottie) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-lottie) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-lottie) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-lottie) (2024.2.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.63->streamlit-lottie) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=0.63->streamlit-lottie) (2.16.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-lottie) (5.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=0.63->streamlit-lottie) (2.1.5)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-lottie) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-lottie) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-lottie) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=0.63->streamlit-lottie) (0.18.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=0.63->streamlit-lottie) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit>=0.63->streamlit-lottie) (1.16.0)\n","Installing collected packages: streamlit-lottie\n","Successfully installed streamlit-lottie-0.0.5\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"]}]},{"cell_type":"code","source":["pip install shap"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z1YKPwW-grsf","executionInfo":{"status":"ok","timestamp":1716818241914,"user_tz":-120,"elapsed":6564,"user":{"displayName":"Pilar Schümmer","userId":"07078196375778711023"}},"outputId":"9828b9ac-20bd-48f2-fbe1-93a8dfe16cda"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.45.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.0.3)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.4)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.0)\n","Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.8)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n"]}]},{"cell_type":"code","source":["!streamlit run app.py & npx localtunnel --port 8501"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bwmDXKcR485k","executionInfo":{"status":"ok","timestamp":1716829978457,"user_tz":-120,"elapsed":5839168,"user":{"displayName":"Pilar Schümmer","userId":"07078196375778711023"}},"outputId":"acd082ad-51de-4e07-a9ae-b89a137d5ae2"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n","\u001b[0m\n","\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n","\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.148.253.209:8501\u001b[0m\n","\u001b[0m\n","\u001b[K\u001b[?25hnpx: installed 22 in 2.143s\n","your url is: https://fresh-dots-smash.loca.lt\n","2024-05-27 16:27:20.467858: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-27 16:27:20.467923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-27 16:27:20.469593: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-05-27 16:27:20.479788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-27 16:27:21.914611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Epoch 1/20\n","25/25 [==============================] - 6s 49ms/step - loss: 0.0276 - val_loss: 0.0113\n","Epoch 2/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0145 - val_loss: 0.0111\n","Epoch 3/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0144 - val_loss: 0.0109\n","Epoch 4/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0143 - val_loss: 0.0113\n","Epoch 5/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0143 - val_loss: 0.0110\n","Epoch 6/20\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0143 - val_loss: 0.0104\n","Epoch 7/20\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0142 - val_loss: 0.0104\n","Epoch 8/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0144 - val_loss: 0.0104\n","Epoch 9/20\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0145 - val_loss: 0.0106\n","Epoch 10/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0144 - val_loss: 0.0103\n","Epoch 11/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0142 - val_loss: 0.0112\n","Epoch 12/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0144 - val_loss: 0.0106\n","Epoch 13/20\n","25/25 [==============================] - 0s 14ms/step - loss: 0.0145 - val_loss: 0.0105\n","Epoch 14/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0143 - val_loss: 0.0104\n","Epoch 15/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0143 - val_loss: 0.0105\n","Epoch 16/20\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0141 - val_loss: 0.0107\n","Epoch 17/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0147 - val_loss: 0.0107\n","Epoch 18/20\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0144 - val_loss: 0.0109\n","Epoch 19/20\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0143 - val_loss: 0.0106\n","Epoch 20/20\n","25/25 [==============================] - 0s 14ms/step - loss: 0.0141 - val_loss: 0.0106\n","7/7 [==============================] - 0s 4ms/step - loss: 0.0106\n","7/7 [==============================] - 1s 4ms/step\n","Epoch 1/10\n","25/25 [==============================] - 6s 48ms/step - loss: 0.0435 - val_loss: 0.0344\n","Epoch 2/10\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0307 - val_loss: 0.0324\n","Epoch 3/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0299 - val_loss: 0.0321\n","Epoch 4/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0296 - val_loss: 0.0317\n","Epoch 5/10\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0291 - val_loss: 0.0313\n","Epoch 6/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0283 - val_loss: 0.0322\n","Epoch 7/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0282 - val_loss: 0.0308\n","Epoch 8/10\n","25/25 [==============================] - 0s 14ms/step - loss: 0.0275 - val_loss: 0.0310\n","Epoch 9/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0271 - val_loss: 0.0309\n","Epoch 10/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0269 - val_loss: 0.0303\n","7/7 [==============================] - 0s 4ms/step - loss: 0.0303\n","7/7 [==============================] - 1s 4ms/step\n","Epoch 1/10\n","25/25 [==============================] - 6s 66ms/step - loss: 0.0279 - val_loss: 0.0388\n","Epoch 2/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0275 - val_loss: 0.0393\n","Epoch 3/10\n","25/25 [==============================] - 0s 14ms/step - loss: 0.0271 - val_loss: 0.0392\n","Epoch 4/10\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0271 - val_loss: 0.0394\n","Epoch 5/10\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0268 - val_loss: 0.0396\n","Epoch 6/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0268 - val_loss: 0.0394\n","Epoch 7/10\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0266 - val_loss: 0.0393\n","Epoch 8/10\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0267 - val_loss: 0.0401\n","Epoch 9/10\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0268 - val_loss: 0.0398\n","Epoch 10/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0267 - val_loss: 0.0397\n","7/7 [==============================] - 0s 6ms/step - loss: 0.0397\n","7/7 [==============================] - 1s 3ms/step\n","Epoch 1/10\n","25/25 [==============================] - 6s 71ms/step - loss: 0.0087 - val_loss: 0.0044\n","Epoch 2/10\n","25/25 [==============================] - 1s 21ms/step - loss: 0.0076 - val_loss: 0.0044\n","Epoch 3/10\n","25/25 [==============================] - 1s 20ms/step - loss: 0.0076 - val_loss: 0.0044\n","Epoch 4/10\n","25/25 [==============================] - 1s 21ms/step - loss: 0.0076 - val_loss: 0.0044\n","Epoch 5/10\n","25/25 [==============================] - 1s 21ms/step - loss: 0.0077 - val_loss: 0.0044\n","Epoch 6/10\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0077 - val_loss: 0.0044\n","Epoch 7/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0076 - val_loss: 0.0044\n","Epoch 8/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0077 - val_loss: 0.0044\n","Epoch 9/10\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0076 - val_loss: 0.0044\n","Epoch 10/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0076 - val_loss: 0.0044\n","7/7 [==============================] - 0s 5ms/step - loss: 0.0044\n","7/7 [==============================] - 1s 3ms/step\n","Epoch 1/10\n","25/25 [==============================] - 5s 48ms/step - loss: 0.0094 - val_loss: 0.0038\n","Epoch 2/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0093 - val_loss: 0.0039\n","Epoch 3/10\n","25/25 [==============================] - 0s 13ms/step - loss: 0.0093 - val_loss: 0.0038\n","Epoch 4/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0093 - val_loss: 0.0038\n","Epoch 5/10\n","25/25 [==============================] - 0s 12ms/step - loss: 0.0094 - val_loss: 0.0038\n","Epoch 6/10\n","25/25 [==============================] - 0s 19ms/step - loss: 0.0093 - val_loss: 0.0039\n","Epoch 7/10\n","25/25 [==============================] - 0s 19ms/step - loss: 0.0093 - val_loss: 0.0039\n","Epoch 8/10\n","25/25 [==============================] - 0s 19ms/step - loss: 0.0093 - val_loss: 0.0039\n","Epoch 9/10\n","25/25 [==============================] - 1s 22ms/step - loss: 0.0094 - val_loss: 0.0038\n","Epoch 10/10\n","25/25 [==============================] - 0s 20ms/step - loss: 0.0092 - val_loss: 0.0043\n","7/7 [==============================] - 0s 4ms/step - loss: 0.0043\n","7/7 [==============================] - 1s 3ms/step\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but SVC was fitted without feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but SVC was fitted without feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but SVC was fitted without feature names\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but SVC was fitted without feature names\n","  warnings.warn(\n","\u001b[34m  Stopping...\u001b[0m\n","^C\n"]}]},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import IsolationForest\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","\n","\n","st.title(\"Anomaly Detection System\")\n","st.image(\"/content/logo.png\",  width=200)\n","\n","\n","metric_columns = ['bandwidth','throughput', 'congestion', 'packet_loss', 'latency', 'jitter']\n","def detect_anomalies_iqr(data, metric, threshold=1.2):\n","    Q1 = data[metric].quantile(0.25)\n","    Q3 = data[metric].quantile(0.75)\n","    IQR = Q3 - Q1\n","    lower_bound = Q1 - threshold * IQR\n","    upper_bound = Q3 + threshold * IQR\n","    return (data[metric] < lower_bound) | (data[metric] > upper_bound)\n","\n","#@st.cache\n","@st.cache_data\n","#@st.cache_data()\n","def load_data():\n","    df = pd.read_csv('/content/dataset.csv')\n","    df['timestamp'] = pd.to_datetime(df['timestamp'])\n","    return df\n","\n","df = load_data()\n","\n","option = st.selectbox(\"Choose an option:\", (\"Detection\", \"Prediction\"),index=None,placeholder=\"Select method...\",)\n","st.write(\"You selected:\",option)\n","\n","if option == \"Detection\":\n","\n","  st.subheader(\"Insert new network data\")\n","  new_data = {}\n","  for metric in metric_columns:\n","      new_value = st.text_input(f'{metric}:')\n","      new_data[metric] = float(new_value) if new_value else None\n","\n","  if st.button('Add data',type=\"primary\"):\n","      with st.expander(\"Anomaly threshold\"):\n","          last_timestamp = df['timestamp'].max()\n","          new_data['timestamp'] = last_timestamp + pd.Timedelta(seconds=180)\n","          new_data_df = pd.DataFrame([new_data], index=[1001])\n","          df = pd.concat([df, new_data_df])\n","\n","          for metric in metric_columns:\n","                df[f'anomaly_{metric}'] = 0\n","\n","          for metric in metric_columns:\n","                df.loc[detect_anomalies_iqr(df, metric), f'anomaly_{metric}'] = 1\n","\n","\n","\n","                fig, ax = plt.subplots(figsize=(12, 6))\n","                ax.plot(df['timestamp'], df[metric], label=metric)\n","                ax.scatter(df[df[f'anomaly_{metric}'] == 1]['timestamp'], df[df[f'anomaly_{metric}'] == 1][metric], color='red', label='Anomaly')\n","                ax.set_title(f'{metric} with Anomalies (IQR)')\n","                ax.set_ylabel(f'{metric} Values')\n","                ax.set_xlabel('Timestamp')\n","                ax.legend()\n","                st.pyplot(fig)\n","\n","\n","          df['anomaly'] = (df[[f'anomaly_{metric}' for metric in metric_columns]].sum(axis=1) >= 2).astype(int)\n","\n","          df['-'] = df['anomaly'].apply(lambda x: ' ' if x == 0 else '❌')\n","\n","\n","      with st.expander('Labeled data'):\n","        st.write(df)\n","        percentage_anomalies = (df['anomaly'].sum() / len(df)) * 100\n","        st.write(\"Anomaly percentage:\", percentage_anomalies, \"%\")\n","\n","\n","      with st.expander(\"Logistic Regression Classification\"):\n","        import pandas as pd\n","        from imblearn.over_sampling import SMOTE\n","        from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n","        from sklearn.preprocessing import StandardScaler\n","        from sklearn.linear_model import LogisticRegression\n","        from sklearn.ensemble import RandomForestClassifier\n","        from sklearn.cluster import KMeans\n","        from sklearn.metrics import classification_report, confusion_matrix\n","        import matplotlib.pyplot as plt\n","\n","\n","        df = pd.read_csv('dataset_label_finalll.csv')\n","\n","\n","        df.index = df['timestamp']\n","\n","        metric_columns = ['bandwidth', 'throughput', 'congestion', 'packet_loss', 'latency', 'jitter', 'anomaly']\n","        df = df[metric_columns]\n","\n","\n","        X = df.drop('anomaly', axis=1)\n","        y = df['anomaly']\n","        smote = SMOTE(random_state=42)\n","        X, y = smote.fit_resample(X, y)\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","\n","        scaler = StandardScaler()\n","        X_train_scaled = scaler.fit_transform(X_train)\n","        X_test_scaled = scaler.transform(X_test)\n","\n","\n","        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","\n","        logistic_regression_model = LogisticRegression()\n","\n","        logistic_regression_cv_scores = cross_val_score(logistic_regression_model, X_train_scaled, y_train, cv=kf, scoring='accuracy')\n","\n","        st.write(\"Logistic Regression Accuracy:\", logistic_regression_cv_scores.mean())\n","\n","\n","        logistic_regression_model.fit(X_train_scaled, y_train)\n","        X_test2 = scaler.transform(df.drop('anomaly', axis=1))\n","        predic = scaler.transform(new_data_df.drop('timestamp', axis=1))\n","\n","        logistic_regression_predictions = logistic_regression_model.predict(X_test_scaled)\n","        st.write(\"\\nLogistic Regression Confusion Matrix:\\n\", confusion_matrix(y_test, logistic_regression_predictions))\n","\n","        st.text(classification_report(y_test, logistic_regression_predictions))\n","        logistic_regression_predictions2 = logistic_regression_model.predict(X_test2)\n","        logistic_regression_predic = logistic_regression_model.predict(predic)\n","        logistic_regression_anomaly_percentage = (logistic_regression_predictions2.sum() / len(logistic_regression_predictions2)) * 100\n","        st.write(\"Anomaly percentage detected with Logistic Regression:\", logistic_regression_anomaly_percentage)\n","        if logistic_regression_predic == 1:\n","            st.write(\"The model DETECTED an anomaly in the new input data!\")\n","        else:\n","            st.write(\"The model did NOT DETECT any anomaly in the new input data.\")\n","      with st.expander(\"Random Forest Classification\"):\n","\n","        random_forest_model = RandomForestClassifier()\n","\n","        random_forest_cv_scores = cross_val_score(random_forest_model, X_train, y_train, cv=kf, scoring='accuracy')\n","\n","        st.write(\"Random Forest Accuracy:\", random_forest_cv_scores.mean())\n","\n","\n","        random_forest_model.fit(X_train, y_train)\n","\n","\n","        random_forest_predictions = random_forest_model.predict(X_test)\n","        st.write(\"\\nRandom Forest Confusion Matrix:\\n\", confusion_matrix(y_test, random_forest_predictions))\n","\n","        st.text(classification_report(y_test, random_forest_predictions))\n","        X_test2 = (df.drop('anomaly', axis=1))\n","        predic=(new_data_df.drop('timestamp', axis=1))\n","\n","        random_forest_predictions2 = random_forest_model.predict(X_test2)\n","        random_forest_predic = random_forest_model.predict(predic)\n","        if random_forest_predic == 1:\n","            st.write(\"The model DETECTED an anomaly in the new input data!\")\n","        else:\n","            st.write(\"The model did NOT DETECT any anomaly in the new input data.\")\n","        random_forest_anomaly_percentage = (random_forest_predictions2.sum() / len(random_forest_predictions2)) * 100\n","        st.write(\"Anomaly percentage detected with Random Forest:\", random_forest_anomaly_percentage)\n","\n","\n","      with st.expander(\"SVM Classification\"):\n","        import pandas as pd\n","        from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n","        from sklearn.svm import SVC\n","        from sklearn.metrics import classification_report, confusion_matrix\n","        from sklearn.preprocessing import StandardScaler\n","        from imblearn.over_sampling import SMOTE\n","        import numpy as np\n","\n","\n","        df = pd.read_csv('dataset_label_finalll.csv')\n","\n","\n","        metric_columns = ['bandwidth', 'throughput', 'congestion', 'packet_loss', 'latency', 'jitter', 'anomaly']\n","        df = df[metric_columns]\n","\n","\n","        X = df.drop('anomaly', axis=1)\n","        y = df['anomaly']\n","\n","\n","        scaler = StandardScaler()\n","        X_scaled = scaler.fit_transform(X)\n","\n","\n","        smote = SMOTE(random_state=42)\n","        X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n","\n","\n","        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","\n","        svm_model = SVC(kernel='rbf', C=1, gamma='auto')\n","\n","\n","        cv_scores = cross_val_score(svm_model, X_resampled, y_resampled, cv=kf, scoring='accuracy')\n","\n","\n","        st.write(\"SVM Accuracy:\", cv_scores.mean())\n","\n","\n","        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n","\n","\n","        svm_model.fit(X_train, y_train)\n","\n","\n","        svm_predictions = svm_model.predict(X_test)\n","        columns = X.columns\n","        predic=scaler.fit_transform(new_data_df.drop('timestamp', axis=1))\n","\n","        X_scaled_df = pd.DataFrame(X_scaled, columns=columns)\n","\n","        svm_predictions2 = svm_model.predict(X_scaled_df)\n","        svm_predic=svm_model.predict(predic)\n","\n","        st.write(\"\\nSVM Confusion Matrix:\\n\", confusion_matrix(y_test, svm_predictions))\n","\n","        st.text(classification_report(y_test, svm_predictions))\n","        svm_anomaly_percentage = (np.sum(svm_predictions2) / len(svm_predictions2)) * 100\n","        st.write(\"Anomaly percentage detected with SVM:\", svm_anomaly_percentage)\n","        if svm_predic == 1:\n","            st.write(\"The model DETECTED an anomaly in the new input data!\")\n","        else:\n","            st.write(\"The model did NOT DETECT any anomaly in the new input data.\")\n","\n","\n","elif option == \"Prediction\":\n","  with st.expander(\"LSTM\"):\n","    import numpy as np\n","    import pandas as pd\n","    from sklearn.preprocessing import MinMaxScaler\n","    from tensorflow.keras.models import Sequential\n","    from tensorflow.keras.layers import LSTM, Dense\n","    from sklearn.model_selection import train_test_split\n","\n","\n","    df = pd.read_csv('dataset.csv')\n","\n","\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    scaled_data = scaler.fit_transform(df['throughput'].values.reshape(-1, 1))\n","\n","\n","    def create_dataset(data, time_steps=1):\n","        X, y = [], []\n","        for i in range(len(data) - time_steps):\n","            X.append(data[i:(i + time_steps), 0])\n","            y.append(data[i + time_steps, 0])\n","        return np.array(X), np.array(y)\n","\n","\n","    time_steps = 10\n","\n","\n","    X, y = create_dataset(scaled_data, time_steps)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n","    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n","\n","\n","    model = Sequential([\n","        LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n","        LSTM(units=50),\n","        Dense(units=1)\n","    ])\n","\n","\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","\n","    model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n","\n","\n","    loss = model.evaluate(X_test, y_test)\n","    st.write(\"Loss in the test data:\", loss)\n","    predictions = model.predict(X_test)\n","\n","    predictions = scaler.inverse_transform(predictions)\n","    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n","    from sklearn.metrics import mean_squared_error\n","    mse = mean_squared_error(y_test, predictions)\n","\n","    st.write(\"Mean square error in test data:\", mse)\n","\n","\n","    fig, ax = plt.subplots(figsize=(14, 7))\n","    ax.plot(y_test, label='True')\n","    ax.plot(predictions, label='Predicted')\n","    ax.set_title(\"Predictions vs. true values\")\n","    ax.set_xlabel(\"Index\")\n","    ax.set_ylabel(\"Throughput\")\n","    ax.legend()\n","\n","\n","    st.pyplot(fig)\n","\n","\n","\n","    df = pd.read_csv('dataset.csv')\n","\n","\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    scaled_data = scaler.fit_transform(df['congestion'].values.reshape(-1, 1))\n","\n","\n","    def create_dataset(data, time_steps=1):\n","        X, y = [], []\n","        for i in range(len(data) - time_steps):\n","            X.append(data[i:(i + time_steps), 0])\n","            y.append(data[i + time_steps, 0])\n","        return np.array(X), np.array(y)\n","\n","\n","    time_steps = 10\n","\n","\n","    X, y = create_dataset(scaled_data, time_steps)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n","    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n","\n","\n","    model = Sequential([\n","        LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n","        LSTM(units=50),\n","        Dense(units=1)\n","    ])\n","\n","\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","\n","    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n","\n","\n","    loss = model.evaluate(X_test, y_test)\n","    st.write(\"Loss in the test data:\", loss)\n","\n","\n","    predictions = model.predict(X_test)\n","\n","\n","    predictions = scaler.inverse_transform(predictions)\n","    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n","\n","\n","    from sklearn.metrics import mean_squared_error\n","    mse = mean_squared_error(y_test, predictions)\n","    st.write(\"Mean square error in test data:\", mse)\n","\n","\n","    fig, ax = plt.subplots(figsize=(14, 7))\n","    ax.plot(y_test, label='True')\n","    ax.plot(predictions, label='Predicted')\n","    ax.set_title(\"Predictions vs. true values\")\n","    ax.set_xlabel(\"Index\")\n","    ax.set_ylabel(\"Congestion\")\n","    ax.legend()\n","\n","\n","    st.pyplot(fig)\n","\n","\n","\n","    df = pd.read_csv('dataset.csv')\n","\n","\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    scaled_data = scaler.fit_transform(df['packet_loss'].values.reshape(-1, 1))\n","\n","\n","    def create_dataset(data, time_steps=1):\n","        X, y = [], []\n","        for i in range(len(data) - time_steps):\n","            X.append(data[i:(i + time_steps), 0])\n","            y.append(data[i + time_steps, 0])\n","        return np.array(X), np.array(y)\n","\n","\n","    time_steps = 10\n","\n","\n","    X, y = create_dataset(scaled_data, time_steps)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n","    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n","\n","\n","    model = Sequential([\n","        LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n","        LSTM(units=50),\n","        Dense(units=1)\n","    ])\n","\n","\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","\n","    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n","\n","\n","    loss = model.evaluate(X_test, y_test)\n","    st.write(\"Loss in the test data:\", loss)\n","\n","\n","    predictions = model.predict(X_test)\n","\n","\n","    predictions = scaler.inverse_transform(predictions)\n","    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n","\n","\n","    from sklearn.metrics import mean_squared_error\n","    mse = mean_squared_error(y_test, predictions)\n","    st.write(\"Mean square error in test data:\", mse)\n","\n","\n","    import matplotlib.pyplot as plt\n","\n","\n","\n","    fig, ax = plt.subplots(figsize=(14, 7))\n","    ax.plot(y_test, label='True')\n","    ax.plot(predictions, label='Predicted')\n","    ax.set_title(\"Predictions vs. true values\")\n","    ax.set_xlabel(\"Index\")\n","    ax.set_ylabel(\"Packet loss\")\n","    ax.legend()\n","\n","\n","    st.pyplot(fig)\n","\n","\n","\n","    df = pd.read_csv('dataset.csv')\n","\n","\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    scaled_data = scaler.fit_transform(df['jitter'].values.reshape(-1, 1))\n","\n","\n","    def create_dataset(data, time_steps=1):\n","        X, y = [], []\n","        for i in range(len(data) - time_steps):\n","            X.append(data[i:(i + time_steps), 0])\n","            y.append(data[i + time_steps, 0])\n","        return np.array(X), np.array(y)\n","\n","\n","    time_steps = 10\n","\n","\n","    X, y = create_dataset(scaled_data, time_steps)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n","    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n","\n","\n","    model = Sequential([\n","        LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n","        LSTM(units=50),\n","        Dense(units=1)\n","    ])\n","\n","\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","\n","    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n","\n","\n","    loss = model.evaluate(X_test, y_test)\n","    st.write(\"Loss in the test data:\", loss)\n","\n","\n","\n","    predictions = model.predict(X_test)\n","\n","\n","    predictions = scaler.inverse_transform(predictions)\n","    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n","\n","\n","    from sklearn.metrics import mean_squared_error\n","    mse = mean_squared_error(y_test, predictions)\n","    st.write(\"Mean square error in test data:\", mse)\n","\n","\n","    fig, ax = plt.subplots(figsize=(14, 7))\n","    ax.plot(y_test, label='True')\n","    ax.plot(predictions, label='Predicted')\n","    ax.set_title(\"Predictions vs. true values\")\n","    ax.set_xlabel(\"Index\")\n","    ax.set_ylabel(\"Jitter\")\n","    ax.legend()\n","    st.pyplot(fig)\n","\n","\n","    df = pd.read_csv('dataset.csv')\n","\n","\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    scaled_data = scaler.fit_transform(df['latency'].values.reshape(-1, 1))\n","\n","\n","    def create_dataset(data, time_steps=1):\n","        X, y = [], []\n","        for i in range(len(data) - time_steps):\n","            X.append(data[i:(i + time_steps), 0])\n","            y.append(data[i + time_steps, 0])\n","        return np.array(X), np.array(y)\n","\n","\n","    time_steps = 10\n","\n","\n","    X, y = create_dataset(scaled_data, time_steps)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n","    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n","\n","\n","    model = Sequential([\n","        LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n","        LSTM(units=50),\n","        Dense(units=1)\n","    ])\n","\n","\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","\n","    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n","\n","\n","    loss = model.evaluate(X_test, y_test)\n","    st.write(\"Loss in the test data:\", loss)\n","\n","\n","    predictions = model.predict(X_test)\n","\n","\n","    predictions = scaler.inverse_transform(predictions)\n","    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n","\n","\n","    from sklearn.metrics import mean_squared_error\n","    mse = mean_squared_error(y_test, predictions)\n","    st.write(\"Mean square error in test data:\", mse)\n","\n","\n","    import matplotlib.pyplot as plt\n","\n","\n","    fig, ax = plt.subplots(figsize=(14, 7))\n","    ax.plot(y_test, label='True')\n","    ax.plot(predictions, label='Predicted')\n","    ax.set_title(\"Predictions vs. true values\")\n","    ax.set_xlabel(\"Index\")\n","    ax.set_ylabel(\"Latency\")\n","    ax.legend()\n","    st.pyplot(fig)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4R81gTJGXZc0","executionInfo":{"status":"ok","timestamp":1716824098489,"user_tz":-120,"elapsed":303,"user":{"displayName":"Pilar Schümmer","userId":"07078196375778711023"}},"outputId":"4b458864-1148-4304-d997-5946e5d537f3"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]}]}